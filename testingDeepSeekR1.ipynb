{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7cd21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to explain Newton's Second Law of Motion. Let me see... From what I remember in physics class, there are three laws named after him: first, third, and second. The second one talks about the relationship between force, mass, and acceleration.\n",
      "\n",
      "Hmm, how does that work? So if a force is applied on an object, it causes the object to accelerate. That means more force equals more acceleration or something like that? But I need to explain all of this clearly.\n",
      "\n",
      "Wait, force is equal to mass times acceleration. Oh yeah, F = ma. So when you apply a greater force, the acceleration increases, making the object speed up faster. Or if there's less force, it doesn't accelerate as much.\n",
      "\n",
      "But why is that? Like, how does force cause acceleration? I think it's because of inertia. Inertia is the resistance to change in motion, right? So an object will keep moving at its current speed unless something stops it or makes it go faster. So if there's a force, you need more mass or apply more force to change that inertia.\n",
      "\n",
      "Wait, maybe another way: when you push a box, if it's empty (small mass), it moves slower than the same box full of books (larger mass) under the same force. That makes sense because heavier objects have more inertia and can accelerate faster for the same force.\n",
      "\n",
      "What about friction? Newton said that the net force is equal to the product of mass and acceleration, but what's the net force here? It would be all the forces acting on the object minus any resistive forces like friction. So if I push an object, there might be friction slowing it down, so the net force isn't just my push; it has to account for that.\n",
      "\n",
      "I should also mention how this applies in real life. Like when accelerating cars, you have more force applied to a heavier car than a lighter one, making it accelerate faster. Or when jumping, the momentum from the ground pushes you forward and you jump up because of your inertia until something stops you.\n",
      "\n",
      "Wait, I'm mixing some concepts here. Oh yeah, Newton's laws are all about forces and motion, so they explain how things move and interact in the world around us.\n",
      "\n",
      "I think that covers it. Let me try to put it all together clearly.\n",
      "</think>\n",
      "\n",
      "Newton's Second Law of Motion states that the acceleration of an object is directly proportional to the net force acting on it and inversely proportional to its mass. Mathematically, this is expressed as F = ma, where F is the net force, m is the mass of the object, and a is its acceleration.\n",
      "\n",
      "1. **Force and Acceleration**: A greater force applied to an object results in a higher acceleration, meaning the object speeds up more quickly.\n",
      "\n",
      "2. **Mass and Inertia**: More massive objects have more inertia, which is their resistance to changes in motion. This means that even with the same force, a larger mass accelerates less than a smaller mass.\n",
      "\n",
      "3. **Resistive Forces**: When considering real-world applications, net force includes all forces acting on an object, such as friction. This affects acceleration by reducing it when opposing motion is present.\n",
      "\n",
      "4. **Real-World Applications**:\n",
      "   - Accelerating vehicles: Heavier vehicles require more force to accelerate the same rate as lighter ones.\n",
      "   - Jumps: The momentum from the ground provides the necessary force for jumping forward and upward due to inertia.\n",
      "\n",
      "In summary, Newton's Second Law explains how the motion of objects is governed by forces, mass, and acceleration, providing a fundamental understanding of how the world moves.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain Newton's second law of motion\"},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b0bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import requests \n",
    "from typing import Any, Dict, List, Mapping, Optional, Union, Iterator\n",
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage, \n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Any, Dict, List, Mapping, Optional, Union, Iterator\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "\n",
    "class OllamaChat(BaseChatModel):\n",
    "    \"\"\"Chat model implementation for Ollama API.\"\"\"\n",
    "    \n",
    "    model_name: str = \"deepseek-r1\"\n",
    "    temperature: float = 0.0\n",
    "    streaming: bool = False\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    \n",
    "    def _convert_to_ollama_messages(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Convert LangChain messages to Ollama message format.\"\"\"\n",
    "        ollama_messages = []\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                role = \"user\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                role = \"assistant\"\n",
    "            elif isinstance(message, SystemMessage):\n",
    "                role = \"system\"\n",
    "            else:\n",
    "                role = \"user\"  # Default fallback\n",
    "                \n",
    "            ollama_messages.append({\n",
    "                \"role\": role,\n",
    "                \"content\": message.content\n",
    "            })\n",
    "        return ollama_messages\n",
    "    \n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Generate a chat response using the Ollama API.\"\"\"\n",
    "        # Convert LangChain messages to Ollama format\n",
    "        ollama_messages = self._convert_to_ollama_messages(messages)\n",
    "        \n",
    "        # Prepare the API request payload\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": ollama_messages,\n",
    "            \"stream\": False,  # We'll handle streaming separately\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        \n",
    "        # Add stop sequences if provided\n",
    "        if stop:\n",
    "            payload[\"stop\"] = stop\n",
    "            \n",
    "        # Add any additional parameters\n",
    "        for key, value in kwargs.items():\n",
    "            payload[key] = value\n",
    "            \n",
    "        # Make the API request\n",
    "        endpoint = f\"{self.base_url}/api/chat\"\n",
    "        response = requests.post(endpoint, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "        \n",
    "        # Create ChatGeneration object\n",
    "        generation = ChatGeneration(\n",
    "            message=AIMessage(content=result[\"message\"][\"content\"]),\n",
    "            generation_info={\"model\": self.model_name, \n",
    "                           \"finish_reason\": result.get(\"finish_reason\", \"stop\")}\n",
    "        )\n",
    "        \n",
    "        # Return the ChatResult\n",
    "        return ChatResult(generations=[generation])\n",
    "    \n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGeneration]:\n",
    "        \"\"\"Stream the chat response from Ollama API.\"\"\"\n",
    "        if not self.streaming:\n",
    "            yield next(iter(self._generate(messages, stop, run_manager, **kwargs).generations))\n",
    "            return\n",
    "            \n",
    "        # Convert LangChain messages to Ollama format\n",
    "        ollama_messages = self._convert_to_ollama_messages(messages)\n",
    "        \n",
    "        # Prepare the API request payload\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": ollama_messages,\n",
    "            \"stream\": True,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        \n",
    "        # Add stop sequences if provided\n",
    "        if stop:\n",
    "            payload[\"stop\"] = stop\n",
    "            \n",
    "        # Add any additional parameters\n",
    "        for key, value in kwargs.items():\n",
    "            payload[key] = value\n",
    "            \n",
    "        # Make the API request\n",
    "        endpoint = f\"{self.base_url}/api/chat\"\n",
    "        response = requests.post(endpoint, data=json.dumps(payload), stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Stream the response\n",
    "        content = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                chunk = json.loads(line)\n",
    "                content_chunk = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                content += content_chunk\n",
    "                \n",
    "                # Create a generation with the accumulated content\n",
    "                generation = ChatGeneration(\n",
    "                    message=AIMessage(content=content),\n",
    "                    generation_info={\"model\": self.model_name}\n",
    "                )\n",
    "                \n",
    "                # Yield the generation\n",
    "                if run_manager:\n",
    "                    run_manager.on_llm_new_token(content_chunk)\n",
    "                yield generation\n",
    "                \n",
    "                # Check if done\n",
    "                if chunk.get(\"done\", False):\n",
    "                    break\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return the type of LLM.\"\"\"\n",
    "        return \"ollama-chat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticRag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
