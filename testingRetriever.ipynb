{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545e95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "PERSIST_DIRECTORY = \"VectorDBs\\\\BAAIbgeLargeEn3BooksVectorDB\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7130fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector database from VectorDBs\\BAAIbgeLargeEn3BooksVectorDB\n",
      "Vector database loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class LoggingRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Custom retriever that logs retrieved documents and filters duplicates.\n",
    "\n",
    "    Attributes:\n",
    "        base_retriever: The base retriever to delegate retrieval to\n",
    "        seen_hashes: A set to track already seen document hashes\n",
    "    \"\"\"\n",
    "    base_retriever: BaseRetriever = Field(...)\n",
    "    seen_hashes: set = Field(default_factory=set)\n",
    "\n",
    "    def _get_relevant_documents(self, query, *, run_manager=None):\n",
    "        \"\"\"\n",
    "        Retrieve and log relevant documents for a given query.\n",
    "\n",
    "        Args:\n",
    "            query: User input query\n",
    "            run_manager: Optional run manager for logging\n",
    "\n",
    "        Returns:\n",
    "            List of unique and relevant documents\n",
    "        \"\"\"\n",
    "        docs = self.base_retriever._get_relevant_documents(query, run_manager=run_manager)\n",
    "        unique_docs = []\n",
    "        docs_with_metadata = []\n",
    "        for doc in docs:\n",
    "            # Create a unique hash using content and metadata\n",
    "            doc_hash = hash(f\"{doc.page_content}-{doc.metadata}\")\n",
    "            if doc_hash not in self.seen_hashes:\n",
    "                self.seen_hashes.add(doc_hash)\n",
    "                unique_docs.append(doc)\n",
    "\n",
    "                source = os.path.basename(doc.metadata.get('source', 'unknown'))\n",
    "                page = doc.metadata.get('page', 'unknown')\n",
    "                docs_with_metadata.append({\"doc\": doc, \"source\": source, \"page\": page})\n",
    "\n",
    "                # print(f\"Retrieved: {source} p.{page} - {doc.page_content[:50]}...\")\n",
    "\n",
    "        global finalContext \n",
    "        finalContext = docs_with_metadata\n",
    "        print(f\"{finalContext=}\")\n",
    "        return unique_docs\n",
    "\n",
    "class RAGAgent:\n",
    "    \"\"\"\n",
    "    RAG (Retrieval-Augmented Generation) Agent class that handles query processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True, numOfContext=3):\n",
    "        \"\"\"\n",
    "        Initialize the agent and set up its components.\n",
    "\n",
    "        Args:\n",
    "            verbose: Whether to print debug information\n",
    "            numOfContext: Number of documents to retrieve for context\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.numOfContext = numOfContext\n",
    "        self.context = []\n",
    "        self.context_failure = 0 \n",
    "\n",
    "        self._load_vector_db()\n",
    "        self._setup_retriever()\n",
    "\n",
    "    def _load_vector_db(self):\n",
    "        \"\"\"Load vector database from disk.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Loading vector database from {PERSIST_DIRECTORY}\")\n",
    "        \n",
    "        embedding_function = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n",
    "\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"rag-chroma\",\n",
    "            embedding_function=embedding_function,\n",
    "            persist_directory=PERSIST_DIRECTORY\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Vector database loaded successfully\")\n",
    "\n",
    "    def _setup_retriever(self):\n",
    "        \"\"\"Configure retriever with Maximal Marginal Relevance and logging.\"\"\"\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": self.numOfContext,\n",
    "                \"fetch_k\": 20,\n",
    "                \"lambda_mult\": 0.8\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.logging_retriever = LoggingRetriever(base_retriever=retriever)\n",
    "\n",
    "        self.retriever_tool = create_retriever_tool(\n",
    "            self.logging_retriever,\n",
    "            \"retrieve_relevant_section\",\n",
    "            \"Search and return information from the documents\"\n",
    "        )\n",
    "\n",
    "        self.tools = [self.retriever_tool]\n",
    "\n",
    "rag_agent = RAGAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaaaf042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalContext=[{'doc': Document(id='d5086f00-d3ae-44cc-81c5-1833e419ab19', metadata={'creationdate': '2016-07-22T19:38:38+05:30', 'creator': 'Adobe InDesign CS6 (Windows)', 'moddate': '2016-07-23T18:53:00+05:30', 'page': 17, 'page_label': 'vii', 'producer': 'Adobe PDF Library 10.0.1', 'source': '3books\\\\ADVANCED_MACHINE_LEARNING_WITH_PYTHON.pdf', 'total_pages': 278, 'trapped': '/False'}, page_content=\"Preface\\n[  vii ]\\nAt times, this book won't be able to give a subject the attention that it deserves. \\nWe cover a lot of ground in this book and the pace is fairly brisk as a result! At \\nthe end of each chapter, I refer you to further reading, in a book or online article, \\nso that you can build a broader base of relevant knowledge. I'd suggest that it's \\nworth doing additional reading around any unfamiliar concept that comes up as \\nyou work through this book, as machine learning knowledge tends to tie together \\nsynergistically; the more you have, the more readily you'll understand new concepts \\nas you expand your toolkit.\\nThis concept of expanding a toolkit of skills is fundamental to what I've tried to \\nachieve with this book. Each chapter introduces one or multiple algorithms and \\nlooks to achieve several goals:\\n• Explaining at a high level what the algorithm does, what problems it'll solve \\nwell, and how you should expect to apply it\\n• Walking through key components of the algorithm, including topology, \\nlearning method, and performance measurement\\n• Identifying how to improve performance by reviewing model output\\nBeyond the transfer of knowledge and practical skills, this book looks to achieve a \\nmore important goal; specifically, to discuss and convey some of the qualities that \\nare common to skilled machine learning practitioners. These include creativity, \\ndemonstrated both in the definition of sophisticated architectures and problem-\\nspecific cleaning techniques. Rigor is another key quality, emphasized throughout \\nthis book by a focus on measuring performance against meaningful targets and \\ncritically assessing early efforts.\\nFinally, this book makes no effort to obscure the realities of working on solving \\ndata challenges: the mixed results of early trials, large iteration counts, and frequent \\nimpasses. Yet at the same time, using a mixture of toy examples, dissection of expert \\napproaches and, toward the end of the book, more real-world challenges, we show \\nhow a creative, tenacious, and rigorous approach can break down these barriers and \\ndeliver meaningful results.\\nAs we proceed, I wish you the best of luck and encourage you to enjoy yourself as \\nyou go, tackling the content prepared for you and applying what you've learned to \\nnew domains or data.\\nLet's get started!\"), 'source': 'ADVANCED_MACHINE_LEARNING_WITH_PYTHON.pdf', 'page': 17}, {'doc': Document(id='03837afd-c5f5-4f16-b280-4e1aa9bdb996', metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 6, 'page_label': 'v', 'producer': 'PyPDF', 'source': '3books\\\\AI_and_Machine_Learning_for_Coders_A_Programmers_Guide_to_Artificial.pdf', 'total_pages': 390}, page_content='Recognizing Clothing Items                                                                                         21\\nThe Data: Fashion MNIST                                                                                        22\\nNeurons for Vision                                                                                                        23\\nDesigning the Neural Network                                                                                    25\\nThe Complete Code                                                                                                   26\\nTraining the Neural Network                                                                                       29\\nv'), 'source': 'AI_and_Machine_Learning_for_Coders_A_Programmers_Guide_to_Artificial.pdf', 'page': 6}, {'doc': Document(id='9921ff17-70c9-45dc-b532-35adcd2610c9', metadata={'author': 'Nithin Buduma & Nikhil Buduma & Joe Papa', 'creationdate': '2022-03-29T20:43:55+00:00', 'creator': 'calibre (5.36.0) [http://calibre-ebook.com]', 'moddate': '2022-03-29T22:43:56+02:00', 'page': 328, 'page_label': '329', 'producer': 'calibre (5.36.0) [http://calibre-ebook.com]', 'source': '3books\\\\Fundamentals of Deep Learning, 2nd Edition.pdf', 'title': 'Fundamentals of Deep Learning, 2nd Edition', 'total_pages': 416}, page_content='forest rather than a single decision tree, we average the information gain for each feature across all of the trees inthe forest and sort using the mean. Note that there is no extra work required in calculating the information gain,since we use information gain to train the individual decision trees in the first place. Thus, we have both example-level interpretability and a global understanding of feature importance for free in tree-based algorithms.\\nLinear Regression\\nA quick background on linear regression: given a set of features and a target variable, our goal is to find the “best”linear combination of features that approximates the target variable. Implicit in this model is the assumption thatthe input features are linearly related to the target variable. We define “best” as the set of coefficients that results inthe linear combination with the lowest root mean squared error when compared against the ground truth:\\ny=β⋅x+ϵ,ϵ∼N(0,σ2)\\nWhere β represents the vector of coefficients. Our built-in, global notion of feature importance follows directlyfrom this. The features that correspond with the coefficients with the highest magnitude are, globally, the mostimportant features in the regression.\\nHow about an example-level notion of feature importance? Recall that to get a prediction for a given example, wetake the dot product between the example and the learned coefficients. Logically, the feature associated with thefeature-coefficient product that contributes the most, in magnitude, to the final result is the feature that is mostimportant for prediction. Without much effort, we have both an example-level and global-level notion ofinterpretability more or less built into linear regression.\\nHowever, linear regression has some unaddressed issues when considering feature importance. For example, whenthere exist significant correlations between the features in a multivariate regression, it is often difficult for themodel to disentangle the effects of these correlated features on the output. In the section on SHAP, we willdescribe Shapley values, which were designed to measure the marginal, unbiased impact of a given feature on theoutput in such cases.\\nMethods for Evaluating Feature Importance\\nFor models where feature importance isn’t built in, there are a variety of methods researchers have developed overthe years to evaluate feature importance. In this section, we will discuss a few that are used in the industry, inaddition to their benefits and shortcomings.\\nPermutation Feature Importance\\nThe idea behind permutation feature importance is quite simple - assume we have a trained neural model f and aset of features U that f has been trained on. We’d like to understand the impact that an individual feature s has onthe predictions of f. One way to do this is to randomly rearrange the values that s takes on in the dataset amongstall of the examples and measure the resulting decrease in predictive accuracy. If it were the case that the feature sdid not add much predictive accuracy in the first place, we should see that the predictive accuracy of f decreasesminimally when using the permuted samples. Inversely, in the case that feature s was predictive of the output in thefirst place, we should see a large drop in predictive accuracy upon permuting the values of s in the dataset. Inessence, if the feature s was originally strongly correlated with the true labels, randomizing the values of s wouldbreak this strong correlation and nullify its effectiveness at predicting the true label.\\nUnfortunately, as with all interpretability methods, this one is not perfect. Imagine the scenario in which our targetis ice cream sales in a given region and two features in U are the readings of two temperature sensors within a onemile radius of each other. We’d expect that each of these features is independently quite predictive of ice creamsales due to the seasonality of our target. However, if we were to perform the permutation methodology presentedabove on this dataset, we’d counterintuitively get a low feature importance for both of these features. Why is thisthe case? Note that although each of these features is strongly predictive of the target, they are also stronglycorrelated due to the close proximity of the two temperature sensors. Additionally, permuting only one of these'), 'source': 'Fundamentals of Deep Learning, 2nd Edition.pdf', 'page': 328}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='d5086f00-d3ae-44cc-81c5-1833e419ab19', metadata={'creationdate': '2016-07-22T19:38:38+05:30', 'creator': 'Adobe InDesign CS6 (Windows)', 'moddate': '2016-07-23T18:53:00+05:30', 'page': 17, 'page_label': 'vii', 'producer': 'Adobe PDF Library 10.0.1', 'source': '3books\\\\ADVANCED_MACHINE_LEARNING_WITH_PYTHON.pdf', 'total_pages': 278, 'trapped': '/False'}, page_content=\"Preface\\n[  vii ]\\nAt times, this book won't be able to give a subject the attention that it deserves. \\nWe cover a lot of ground in this book and the pace is fairly brisk as a result! At \\nthe end of each chapter, I refer you to further reading, in a book or online article, \\nso that you can build a broader base of relevant knowledge. I'd suggest that it's \\nworth doing additional reading around any unfamiliar concept that comes up as \\nyou work through this book, as machine learning knowledge tends to tie together \\nsynergistically; the more you have, the more readily you'll understand new concepts \\nas you expand your toolkit.\\nThis concept of expanding a toolkit of skills is fundamental to what I've tried to \\nachieve with this book. Each chapter introduces one or multiple algorithms and \\nlooks to achieve several goals:\\n• Explaining at a high level what the algorithm does, what problems it'll solve \\nwell, and how you should expect to apply it\\n• Walking through key components of the algorithm, including topology, \\nlearning method, and performance measurement\\n• Identifying how to improve performance by reviewing model output\\nBeyond the transfer of knowledge and practical skills, this book looks to achieve a \\nmore important goal; specifically, to discuss and convey some of the qualities that \\nare common to skilled machine learning practitioners. These include creativity, \\ndemonstrated both in the definition of sophisticated architectures and problem-\\nspecific cleaning techniques. Rigor is another key quality, emphasized throughout \\nthis book by a focus on measuring performance against meaningful targets and \\ncritically assessing early efforts.\\nFinally, this book makes no effort to obscure the realities of working on solving \\ndata challenges: the mixed results of early trials, large iteration counts, and frequent \\nimpasses. Yet at the same time, using a mixture of toy examples, dissection of expert \\napproaches and, toward the end of the book, more real-world challenges, we show \\nhow a creative, tenacious, and rigorous approach can break down these barriers and \\ndeliver meaningful results.\\nAs we proceed, I wish you the best of luck and encourage you to enjoy yourself as \\nyou go, tackling the content prepared for you and applying what you've learned to \\nnew domains or data.\\nLet's get started!\"),\n",
       " Document(id='03837afd-c5f5-4f16-b280-4e1aa9bdb996', metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 6, 'page_label': 'v', 'producer': 'PyPDF', 'source': '3books\\\\AI_and_Machine_Learning_for_Coders_A_Programmers_Guide_to_Artificial.pdf', 'total_pages': 390}, page_content='Recognizing Clothing Items                                                                                         21\\nThe Data: Fashion MNIST                                                                                        22\\nNeurons for Vision                                                                                                        23\\nDesigning the Neural Network                                                                                    25\\nThe Complete Code                                                                                                   26\\nTraining the Neural Network                                                                                       29\\nv'),\n",
       " Document(id='9921ff17-70c9-45dc-b532-35adcd2610c9', metadata={'author': 'Nithin Buduma & Nikhil Buduma & Joe Papa', 'creationdate': '2022-03-29T20:43:55+00:00', 'creator': 'calibre (5.36.0) [http://calibre-ebook.com]', 'moddate': '2022-03-29T22:43:56+02:00', 'page': 328, 'page_label': '329', 'producer': 'calibre (5.36.0) [http://calibre-ebook.com]', 'source': '3books\\\\Fundamentals of Deep Learning, 2nd Edition.pdf', 'title': 'Fundamentals of Deep Learning, 2nd Edition', 'total_pages': 416}, page_content='forest rather than a single decision tree, we average the information gain for each feature across all of the trees inthe forest and sort using the mean. Note that there is no extra work required in calculating the information gain,since we use information gain to train the individual decision trees in the first place. Thus, we have both example-level interpretability and a global understanding of feature importance for free in tree-based algorithms.\\nLinear Regression\\nA quick background on linear regression: given a set of features and a target variable, our goal is to find the “best”linear combination of features that approximates the target variable. Implicit in this model is the assumption thatthe input features are linearly related to the target variable. We define “best” as the set of coefficients that results inthe linear combination with the lowest root mean squared error when compared against the ground truth:\\ny=β⋅x+ϵ,ϵ∼N(0,σ2)\\nWhere β represents the vector of coefficients. Our built-in, global notion of feature importance follows directlyfrom this. The features that correspond with the coefficients with the highest magnitude are, globally, the mostimportant features in the regression.\\nHow about an example-level notion of feature importance? Recall that to get a prediction for a given example, wetake the dot product between the example and the learned coefficients. Logically, the feature associated with thefeature-coefficient product that contributes the most, in magnitude, to the final result is the feature that is mostimportant for prediction. Without much effort, we have both an example-level and global-level notion ofinterpretability more or less built into linear regression.\\nHowever, linear regression has some unaddressed issues when considering feature importance. For example, whenthere exist significant correlations between the features in a multivariate regression, it is often difficult for themodel to disentangle the effects of these correlated features on the output. In the section on SHAP, we willdescribe Shapley values, which were designed to measure the marginal, unbiased impact of a given feature on theoutput in such cases.\\nMethods for Evaluating Feature Importance\\nFor models where feature importance isn’t built in, there are a variety of methods researchers have developed overthe years to evaluate feature importance. In this section, we will discuss a few that are used in the industry, inaddition to their benefits and shortcomings.\\nPermutation Feature Importance\\nThe idea behind permutation feature importance is quite simple - assume we have a trained neural model f and aset of features U that f has been trained on. We’d like to understand the impact that an individual feature s has onthe predictions of f. One way to do this is to randomly rearrange the values that s takes on in the dataset amongstall of the examples and measure the resulting decrease in predictive accuracy. If it were the case that the feature sdid not add much predictive accuracy in the first place, we should see that the predictive accuracy of f decreasesminimally when using the permuted samples. Inversely, in the case that feature s was predictive of the output in thefirst place, we should see a large drop in predictive accuracy upon permuting the values of s in the dataset. Inessence, if the feature s was originally strongly correlated with the true labels, randomizing the values of s wouldbreak this strong correlation and nullify its effectiveness at predicting the true label.\\nUnfortunately, as with all interpretability methods, this one is not perfect. Imagine the scenario in which our targetis ice cream sales in a given region and two features in U are the readings of two temperature sensors within a onemile radius of each other. We’d expect that each of these features is independently quite predictive of ice creamsales due to the seasonality of our target. However, if we were to perform the permutation methodology presentedabove on this dataset, we’d counterintuitively get a low feature importance for both of these features. Why is thisthe case? Note that although each of these features is strongly predictive of the target, they are also stronglycorrelated due to the close proximity of the two temperature sensors. Additionally, permuting only one of these')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_agent.logging_retriever._get_relevant_documents(\"feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78a04c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3books\\Fundamentals of Deep Learning, 2nd Edition.pdf\n",
      "3books\\ADVANCED_MACHINE_LEARNING_WITH_PYTHON.pdf\n",
      "3books\\AI_and_Machine_Learning_for_Coders_A_Programmers_Guide_to_Artificial.pdf\n"
     ]
    }
   ],
   "source": [
    "client = rag_agent.vectorstore._collection\n",
    "\n",
    "results  = client.get(include=['metadatas'])\n",
    "\n",
    "sources = set()\n",
    "\n",
    "for metadata in results['metadatas']:\n",
    "    if metadata and 'source' in metadata:\n",
    "        source = metadata['source']\n",
    "        sources.add(source)\n",
    "\n",
    "for source in sources:\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff5c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticRag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
