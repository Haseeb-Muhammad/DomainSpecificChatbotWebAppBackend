{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3176e974",
   "metadata": {},
   "source": [
    "# Testing Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166ef44",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcreatingVectorDB\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorDatabaseManager\n\u001b[1;32m----> 2\u001b[0m db_manager \u001b[38;5;241m=\u001b[39m \u001b[43mVectorDatabaseManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mhasee\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mNCAI\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDomainSpecificChatbotWebAppBackend\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvdb_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mhasee\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mNCAI\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDomainSpecificChatbotWebAppBackend\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mVectorDBs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-small-en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrag-chroma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hasee\\Desktop\\NCAI\\DomainSpecificChatbotWebAppBackend\\creatingVectorDB.py:52\u001b[0m, in \u001b[0;36mVectorDatabaseManager.__init__\u001b[1;34m(self, documents_directory, vdb_directory, model_name, collection_name, chunk_size, chunk_overlap)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Initialize text splitter\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mRecursiveCharacterTextSplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tiktoken_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_overlap\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Vectorstore will be loaded/created when needed\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorstore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasee\\miniconda3\\envs\\AgenticRag\\lib\\site-packages\\langchain_text_splitters\\base.py:212\u001b[0m, in \u001b[0;36mTextSplitter.from_tiktoken_encoder\u001b[1;34m(cls, encoding_name, model_name, allowed_special, disallowed_special, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     extra_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding_name,\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name,\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallowed_special\u001b[39m\u001b[38;5;124m\"\u001b[39m: allowed_special,\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisallowed_special\u001b[39m\u001b[38;5;124m\"\u001b[39m: disallowed_special,\n\u001b[0;32m    209\u001b[0m     }\n\u001b[0;32m    210\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kwargs}\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(length_function\u001b[38;5;241m=\u001b[39m_tiktoken_encoder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasee\\miniconda3\\envs\\AgenticRag\\lib\\site-packages\\langchain_text_splitters\\character.py:73\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter.__init__\u001b[1;34m(self, separators, keep_separator, is_separator_regex, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     67\u001b[0m     separators: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new TextSplitter.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(keep_separator\u001b[38;5;241m=\u001b[39mkeep_separator, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separators \u001b[38;5;241m=\u001b[39m separators \u001b[38;5;129;01mor\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_separator_regex \u001b[38;5;241m=\u001b[39m is_separator_regex\n",
      "File \u001b[1;32mc:\\Users\\hasee\\miniconda3\\envs\\AgenticRag\\lib\\site-packages\\langchain_text_splitters\\base.py:54\u001b[0m, in \u001b[0;36mTextSplitter.__init__\u001b[1;34m(self, chunk_size, chunk_overlap, length_function, keep_separator, add_start_index, strip_whitespace)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m     chunk_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4000\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     strip_whitespace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new TextSplitter.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m                          every document\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchunk_overlap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m:\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot a larger chunk overlap (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_overlap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) than chunk size \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), should be smaller.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m         )\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_size \u001b[38;5;241m=\u001b[39m chunk_size\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from creatingVectorDB import VectorDatabaseManager\n",
    "db_manager = VectorDatabaseManager(\n",
    "        documents_directory=config.DOCUMENTS_DIR,\n",
    "        vdb_directory=config.VDB_DIR,\n",
    "        model_name=config.EMBEDDING_MODEL,\n",
    "        collection_name=config.COLLECTION_NAME,\n",
    "        chunk_size=config.CHUNK_SIZE,\n",
    "        chunk_overlap=config.CHUNK_OVERLAP\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99890ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hasee\\Desktop\\NCAI\\DomainSpecificChatbotWebAppBackend\\creatingVectorDB.py:75: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self._vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing vector database from C:\\Users\\hasee\\Desktop\\NCAI\\VectorDBs\\bge-small-en_test\n",
      "\n",
      "=== Database Statistics ===\n",
      "Total documents: 0\n",
      "Number of PDFs: 0\n",
      "PDF names: []\n",
      "\n",
      "=== PDFs in Database ===\n",
      "PDFs: []\n"
     ]
    }
   ],
   "source": [
    "db_manager.load_existing_database()\n",
    "# db_manager.create_or_update_database()\n",
    "print(\"\\n=== Database Statistics ===\")\n",
    "stats = db_manager.get_database_statistics()\n",
    "print(f\"Total documents: {stats.get('total_documents', 0)}\")\n",
    "print(f\"Number of PDFs: {stats.get('pdf_count', 0)}\")\n",
    "print(f\"PDF names: {stats.get('pdf_names', [])}\")\n",
    "\n",
    "# List PDFs in database\n",
    "print(\"\\n=== PDFs in Database ===\")\n",
    "pdf_names = db_manager.list_pdf_names_in_database()\n",
    "print(f\"PDFs: {list(pdf_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='labels are in{1,2,3,4,5}, so it s a multiclass problem. We can use theone versus rest strategy to convert this multiclass problem into ve binary classi cation problems. 1To be more precise we would addD(D 1) parameters wi,j. 2The notation means much less than. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9' metadata={'author': '', 'chunk_index': 2346, 'cleaned_length': 323, 'creationdate': '2019-04-16T18:30:49-04:00', 'creator': 'LaTeX with hyperref package', 'keywords': '', 'moddate': '2019-05-02T20:11:55-03:00', 'original_length': 329, 'page': 'combined', 'page_label': '1', 'pdf_name': 'the-hundred-page-machine-learning-bookpdf-pdf-free.pdf', 'producer': 'pdfTeX-1.40.19', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) kpathsea version 6.3.0', 'source': '/home/haseebmuhammad/Desktop/AITeacherChatbot/CQADatasetFromBooks/AI-books/the-hundred-page-machine-learning-bookpdf-pdf-free.pdf', 'source_file': '/home/haseebmuhammad/Desktop/AITeacherChatbot/CQADatasetFromBooks/AI-books/the-hundred-page-machine-learning-bookpdf-pdf-free.pdf', 'subject': '', 'title': '', 'total_pages': 145, 'trapped': '/False'}\n",
      "page_content='labels are in{1,2,3,4,5}, so it s a multiclass problem. We can use theone versus rest strategy to convert this multiclass problem into ve binary classi cation problems. 1To be more precise we would addD(D 1) parameters wi,j. 2The notation means much less than. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9' metadata={'author': '', 'chunk_index': 2346, 'cleaned_length': 323, 'creationdate': '2019-04-16T18:30:49-04:00', 'creator': 'LaTeX with hyperref package', 'keywords': '', 'moddate': '2019-05-02T20:11:55-03:00', 'original_length': 329, 'page': 'combined', 'page_label': '1', 'pdf_name': 'the-hundred-page-machine-learning-bookpdf-pdf-free.pdf', 'producer': 'pdfTeX-1.40.19', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) kpathsea version 6.3.0', 'source': '/home/haseebmuhammad/Desktop/AITeacherChatbot/CQADatasetFromBooks/AI-books/the-hundred-page-machine-learning-bookpdf-pdf-free.pdf', 'source_file': '/home/haseebmuhammad/Desktop/AITeacherChatbot/CQADatasetFromBooks/AI-books/the-hundred-page-machine-learning-bookpdf-pdf-free.pdf', 'subject': '', 'title': '', 'total_pages': 145, 'trapped': '/False'}\n",
      "page_content='noise, 6 7 class, 568 nominal attributes, 49 mixture model, 289 numeric prediction, 254 symbols, 49 NominalToBinary filter, 433t 435t, 439, 444, 444t, 471 472 NominalToString filter, 433t 435t, 439 nonlinear class boundaries, 226 227 NonSparseToSparse filter, 441t, 442 normal distribution assumption, 97, 99 confidence limits, 152t normalization, 462 Normalize filter, 433t 435t, 437, 441t, 442 NOT, 233 nuclear family, 44 46 null hypothesis, 158 numeric attributes, 49, 314 322 1R, 87 89 classification rules, 205 converting discrete attributes to, 322 decision tree, 193 194 discretization of, 306 Na ve Bayes, 94 97 normal-distribution assumption for, 99 numeric prediction, 15, 40 additive regression, 362 363 bagging for, 354 355 evaluating, 180 182 linear models, 124 125' metadata={'chunk_index': 8091, 'cleaned_length': 778, 'creationdate': '', 'creator': 'PyPDF', 'original_length': 779, 'page': 'combined', 'page_label': 'cover', 'pdf_name': 'Data Mining - Pratical Machine Learning Tools and Techniques.pdf', 'producer': 'PyPDF', 'source': '/home/haseebmuhammad/Desktop/AITeacherChatbot/CQADatasetFromBooks/AI-books/Data Mining - Pratical Machine Learning Tools and Techniques.pdf', 'source_file': '/home/haseebmuhammad/Desktop/AITeacherChatbot/CQADatasetFromBooks/AI-books/Data Mining - Pratical Machine Learning Tools and Techniques.pdf', 'total_pages': 665, 'trapped': '/False'}\n"
     ]
    }
   ],
   "source": [
    "search_results = db_manager.search_documents(\"classification\", k=3)\n",
    "for doc in search_results:\n",
    "    print(doc)\n",
    "# print(search_results)\n",
    "# print(f\"Found {search_results)} search results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a823001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agenticRetrieverv4 import RAGAgent\n",
    "rag_agent = RAGAgent(numOfContext=3,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd95bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is machine learning?\"\n",
    "print(f\"{'-'*50} {'-'*50}\")\n",
    "print(f\"Question: {question}\")\n",
    "\n",
    "response, context = rag_agent(question)\n",
    "print(f\"Answer: {response}\")\n",
    "print(\"\")\n",
    "\n",
    "    \n",
    "for part_context in context:\n",
    "    print(f\"Book Title: {part_context['book_title']}\")\n",
    "    print(f\"Context: {part_context['page_content']}\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticRag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
