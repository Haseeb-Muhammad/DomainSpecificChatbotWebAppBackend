{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAG Retrieval Agent Application\n",
    "\n",
    "This module implements a Retrieval-Augmented Generation (RAG) system using LangChain and LangGraph.\n",
    "The system can:\n",
    "1. Load and access a locally stored vector database\n",
    "2. Process user queries through an agent that decides whether to retrieve documents\n",
    "3. Grade document relevance and optionally rewrite the query\n",
    "4. Generate final responses based on retrieved documents\n",
    "\n",
    "Usage:\n",
    "    from rag_agent import RAGAgent\n",
    "\n",
    "    # Initialize the agent\n",
    "    rag_agent = RAGAgent()\n",
    "\n",
    "    # Process a query\n",
    "    response = rag_agent.process_query(\"What is strong AI?\")\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import Annotated, Dict, List, Literal, Sequence, Any, Optional\n",
    "\n",
    "# Suppress warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# OpenAI client and Pydantic base classes\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# LangChain components\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# LangGraph components\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Directory for persisting the vector database\n",
    "PERSIST_DIRECTORY = \"BAAIAIBooksListVectorDB\"\n",
    "finalContext = []\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Type definition for the agent's state.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: Sequence of conversation messages\n",
    "    \"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "class LoggingRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Custom retriever that logs retrieved documents and filters duplicates.\n",
    "\n",
    "    Attributes:\n",
    "        base_retriever: The base retriever to delegate retrieval to\n",
    "        seen_hashes: A set to track already seen document hashes\n",
    "    \"\"\"\n",
    "    base_retriever: BaseRetriever = Field(...)\n",
    "    seen_hashes: set = Field(default_factory=set)\n",
    "\n",
    "    def _get_relevant_documents(self, query, *, run_manager=None):\n",
    "        \"\"\"\n",
    "        Retrieve and log relevant documents for a given query.\n",
    "\n",
    "        Args:\n",
    "            query: User input query\n",
    "            run_manager: Optional run manager for logging\n",
    "\n",
    "        Returns:\n",
    "            List of unique and relevant documents\n",
    "        \"\"\"\n",
    "        docs = self.base_retriever._get_relevant_documents(query, run_manager=run_manager)\n",
    "        unique_docs = []\n",
    "        docs_with_metadata = []\n",
    "\n",
    "        for doc in docs:\n",
    "            # Create a unique hash using content and metadata\n",
    "            doc_hash = hash(f\"{doc.page_content}-{doc.metadata}\")\n",
    "            if doc_hash not in self.seen_hashes:\n",
    "                self.seen_hashes.add(doc_hash)\n",
    "                unique_docs.append(doc)\n",
    "\n",
    "                source = os.path.basename(doc.metadata.get('source', 'unknown'))\n",
    "                page = doc.metadata.get('page', 'unknown')\n",
    "                docs_with_metadata.append({\"doc\": doc, \"source\": source, \"page\": page})\n",
    "\n",
    "                print(f\"Retrieved: {source} p.{page} - {doc.page_content[:50]}...\")\n",
    "\n",
    "        global finalContext \n",
    "        finalContext = docs_with_metadata\n",
    "\n",
    "        return unique_docs\n",
    "\n",
    "\n",
    "class RAGAgent:\n",
    "    \"\"\"\n",
    "    RAG (Retrieval-Augmented Generation) Agent class that handles query processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True, numOfContext=3):\n",
    "        \"\"\"\n",
    "        Initialize the agent and set up its components.\n",
    "\n",
    "        Args:\n",
    "            verbose: Whether to print debug information\n",
    "            numOfContext: Number of documents to retrieve for context\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.client = OpenAI()\n",
    "        self.numOfContext = numOfContext\n",
    "        self.context = []\n",
    "\n",
    "        self._load_vector_db()\n",
    "        self._setup_retriever()\n",
    "        self._build_workflow()\n",
    "\n",
    "    def _load_vector_db(self):\n",
    "        \"\"\"Load vector database from disk.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Loading vector database from {PERSIST_DIRECTORY}\")\n",
    "        \n",
    "        embedding_function = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"rag-chroma\",\n",
    "            embedding_function=embedding_function,\n",
    "            persist_directory=PERSIST_DIRECTORY\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Vector database loaded successfully\")\n",
    "\n",
    "    def _setup_retriever(self):\n",
    "        \"\"\"Configure retriever with Maximal Marginal Relevance and logging.\"\"\"\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": self.numOfContext,\n",
    "                \"fetch_k\": 20,\n",
    "                \"lambda_mult\": 0.5\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.logging_retriever = LoggingRetriever(base_retriever=retriever)\n",
    "\n",
    "        self.retriever_tool = create_retriever_tool(\n",
    "            self.logging_retriever,\n",
    "            \"retrieve_relevant_section\",\n",
    "            \"Search and return information from the documents\"\n",
    "        )\n",
    "\n",
    "        self.tools = [self.retriever_tool]\n",
    "\n",
    "    def _build_workflow(self):\n",
    "        \"\"\"Build the LangGraph workflow for query handling.\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "\n",
    "        # Define nodes\n",
    "        workflow.add_node(\"agent\", self._agent)\n",
    "        workflow.add_node(\"retrieve\", ToolNode([self.retriever_tool]))\n",
    "        workflow.add_node(\"rewrite\", self._rewrite)\n",
    "        workflow.add_node(\"generate\", self._generate)\n",
    "\n",
    "        # Define edges\n",
    "        workflow.add_edge(START, \"agent\")\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"agent\",\n",
    "            tools_condition,\n",
    "            {\n",
    "                \"tools\": \"retrieve\",\n",
    "                END: END,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"retrieve\",\n",
    "            self._grade_documents,\n",
    "            {\n",
    "                \"generate\": \"generate\",\n",
    "                \"rewrite\": \"rewrite\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        workflow.add_edge(\"generate\", END)\n",
    "        workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "        self.graph = workflow.compile()\n",
    "\n",
    "    def _agent(self, state):\n",
    "        \"\"\"\n",
    "        Agent node that decides whether to answer directly or use a tool.\n",
    "\n",
    "        Args:\n",
    "            state: Current agent state\n",
    "\n",
    "        Returns:\n",
    "            Updated message list with agent decision\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"---CALL AGENT---\")\n",
    "\n",
    "        messages = state[\"messages\"]\n",
    "        model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\").bind_tools(self.tools)\n",
    "        response = model.invoke(messages)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    def _grade_documents(self, state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "        \"\"\"\n",
    "        Decide if retrieved documents are relevant enough.\n",
    "\n",
    "        Args:\n",
    "            state: Current state including query and context\n",
    "\n",
    "        Returns:\n",
    "            A string to determine the next step in the workflow\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "        class Grade(BaseModel):\n",
    "            binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "        model = ChatOpenAI(temperature=0, model=\"gpt-4o\", streaming=True)\n",
    "        llm_with_tool = model.with_structured_output(Grade)\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "            Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "            Here is the user question: {question} \\n\n",
    "            If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "            Give a binary score 'yes' or 'no' score to indicate whether the question can be answered using the document.\"\"\",\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        )\n",
    "\n",
    "        chain = prompt | llm_with_tool\n",
    "\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        question = messages[0].content\n",
    "        docs = last_message.content\n",
    "\n",
    "        scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "        score = scored_result.binary_score\n",
    "\n",
    "        if score == \"yes\":\n",
    "            global finalContext\n",
    "            self.context = finalContext\n",
    "            if self.verbose:\n",
    "                print(\"---DECISION: DOCS RELEVANT---\")\n",
    "            return \"generate\"\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "            return \"rewrite\"\n",
    "\n",
    "    def _rewrite(self, state):\n",
    "        \"\"\"\n",
    "        Rewrite the user query to improve retrieval results.\n",
    "\n",
    "        Args:\n",
    "            state: Current state containing the original query\n",
    "\n",
    "        Returns:\n",
    "            Updated message list with rewritten query\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"---TRANSFORM QUERY---\")\n",
    "\n",
    "        question = state[\"messages\"][0].content\n",
    "        msg = [\n",
    "            HumanMessage(\n",
    "                content=f\"\"\" \\n \n",
    "        Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Formulate an improved question: \"\"\"\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "        response = model.invoke(msg)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    def _generate(self, state):\n",
    "        \"\"\"\n",
    "        Generate the final answer based on retrieved documents.\n",
    "\n",
    "        Args:\n",
    "            state: Current state containing user query and context\n",
    "\n",
    "        Returns:\n",
    "            Message with the final generated response\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"---GENERATE---\")\n",
    "\n",
    "        question = state[\"messages\"][0].content\n",
    "        docs = state[\"messages\"][-1].content\n",
    "\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True)\n",
    "\n",
    "        formatted_prompt = prompt.format(context=docs, question=question)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Final Prompt:\\n\", formatted_prompt)\n",
    "\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "        response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    def __call__(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Run the RAG agent end-to-end with the provided query.\n",
    "\n",
    "        Args:\n",
    "            query: Input question from the user\n",
    "\n",
    "        Returns:\n",
    "            Response string and list of context metadata\n",
    "        \"\"\"\n",
    "        inputs = {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=query)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        result = None\n",
    "        lastAgent = None\n",
    "\n",
    "        for output in self.graph.stream(inputs):\n",
    "            for key, value in output.items():\n",
    "                print(f\"Output from node '{key}':\")\n",
    "                print(value)\n",
    "                print(\"---\\n\")\n",
    "                result = value\n",
    "                lastAgent = key\n",
    "\n",
    "        if lastAgent == \"agent\":\n",
    "            return result[\"messages\"][0].content, []\n",
    "\n",
    "        extracted_data = []\n",
    "        for entry in self.context:\n",
    "            doc = entry.get(\"doc\", {})\n",
    "            extracted_data.append({\n",
    "                \"book_title\": os.path.basename(doc.metadata[\"source\"]).split(\".\")[0],\n",
    "                \"page_number\": doc.metadata[\"page\"],\n",
    "                \"page_content\": doc.page_content\n",
    "            })\n",
    "\n",
    "        return result[\"messages\"][0], extracted_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = RAGAgent(verbose=True)\n",
    "\n",
    "# Process a query\n",
    "response, context = rag_agent(\"what are stochastic and minibatch gradient descent?\")\n",
    "print(\"\\nFinal Response:\\n\", response)\n",
    "print(\"final COntext:\", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in context:\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.metadata[\"page\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticRag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
